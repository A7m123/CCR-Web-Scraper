# CCR Web Scraper - Commercial Companies Registry

A Python web scraper for extracting data from the Jordanian Commercial Companies Registry website.

## ğŸŒ Target Website

**URL**: https://ccr.mit.gov.jo/mitq/faces/HomePage

**Description**: Automated data extraction tool for commercial company records in Jordan.

---

## âœ¨ Features

- âœ… **Automated Scraping**: Extracts data from all pages automatically
- âœ… **Smart Pagination**: Handles website navigation seamlessly
- âœ… **Checkpoint System**: Saves progress every 10 pages to prevent data loss
- âœ… **Arabic Support**: Properly handles Arabic text and column names
- âœ… **Error Handling**: Robust error recovery and retry mechanisms
- âœ… **Progress Tracking**: Live progress updates in terminal
- âœ… **Multiple Export Formats**: Excel (.xlsx) and CSV support
- âœ… **Configurable**: Easy configuration via JSON file

---

## ğŸ“Š Data Collected

The scraper extracts the following information for each company:

| Column | Arabic Name | Description |
|--------|-------------|-------------|
| Registration Number | Ø±Ù‚Ù… Ø§Ù„ØªØ³Ø¬ÙŠÙ„ | Unique company registration ID |
| Governorate | Ø§Ù„Ù…Ø­Ø§ÙØ¸Ø© | Location/region |
| Registration Date | ØªØ§Ø±ÙŠØ® Ø§Ù„ØªØ³Ø¬ÙŠÙ„ | Date of registration |
| Institution Owner | Ù…Ø§Ù„Ùƒ Ø§Ù„Ù…Ø¤Ø³Ø³Ø© | Owner name |
| Commercial Address | Ø§Ù„Ø¹Ù†ÙˆØ§Ù† Ø§Ù„ØªØ¬Ø§Ø±ÙŠ | Business address |
| Trade Name | Ø§Ù„Ø¥Ø³Ù… Ø§Ù„ØªØ¬Ø§Ø±ÙŠ | Company trade name |
| Current Capital | Ø±Ø£Ø³ Ø§Ù„Ù…Ø§Ù„ Ø§Ù„Ø­Ø§Ù„ÙŠ | Current capital amount |
| Status | Ø§Ù„Ø­Ø§Ù„Ø© | Company status |
| Action | Ø§Ù„Ø¥Ø¬Ø±Ø§Ø¡ | Available actions |

---

## ğŸš€ Quick Start

### Prerequisites

- Python 3.8 or higher
- Google Chrome browser
- Stable internet connection

### Installation

1. **Clone the repository**
```bash
git clone <repository-url>
cd data
```

2. **Install dependencies**
```bash
pip install -r requirements.txt
```

### Basic Usage

**Run the scraper:**
```bash
python scraper_bulk.py
```

That's it! The scraper will start collecting data automatically.

---

## âš™ï¸ Configuration

Edit `config.json` to customize scraping behavior:

```json
{
  "url": "https://ccr.mit.gov.jo/mitq/faces/HomePage",
  "wait_timeout": 10,
  "page_load_delay": 2,
  "pagination": {
    "scrape_all_pages": true,
    "max_pages": 200000
  },
  "output": {
    "format": "excel",
    "filename": "ccr_data_{date}.xlsx"
  }
}
```

### Configuration Options

- **`wait_timeout`**: Maximum seconds to wait for page elements (default: 10)
- **`page_load_delay`**: Seconds to wait between page loads (default: 2)
- **`max_pages`**: Maximum number of pages to scrape (default: 200000)
- **`format`**: Output format - "excel" or "csv"
- **`filename`**: Output filename pattern (use {date} for timestamp)

---

## ğŸ“ˆ Performance

Based on testing:
- **Scraping Rate**: ~50 records per minute
- **Total Records**: ~764,242 records across 152,849 pages
- **Estimated Time**: ~255 hours (~10.6 days) for complete dataset
- **Output File Size**: ~100-150 MB for full dataset

---

## ğŸ“ Output Files

### Automatic Saves

1. **Checkpoint Files** (every 10 pages):
   - `ccr_checkpoint_page10_YYYYMMDD_HHMMSS.xlsx`
   - `ccr_checkpoint_page20_YYYYMMDD_HHMMSS.xlsx`
   - Prevents data loss if scraper is interrupted

2. **Final Output**:
   - `ccr_final_YYYYMMDD_HHMMSS.xlsx`
   - Complete dataset with all records

3. **Interrupted Saves**:
   - `ccr_interrupted_pageXXX_YYYYMMDD_HHMMSS.xlsx`
   - Saved automatically if you stop the scraper (Ctrl+C)

---

## ğŸ› ï¸ Project Structure

```
data/
â”œâ”€â”€ scraper_bulk.py          # Main production scraper
â”œâ”€â”€ scraper_final.py         # Alternative scraper version
â”œâ”€â”€ config.json              # Configuration file
â”œâ”€â”€ requirements.txt         # Python dependencies
â”œâ”€â”€ view_data.py             # Data viewer utility
â”œâ”€â”€ README.md                # This file
â””â”€â”€ START_HERE.md            # Detailed usage guide
```

---

## ğŸ“– Usage Examples

### Example 1: Test Run (100 pages)

```bash
# Edit config.json, set "max_pages": 100
python scraper_bulk.py
```

### Example 2: Partial Scrape (10,000 pages)

```bash
# Edit config.json, set "max_pages": 10000
python scraper_bulk.py
```

### Example 3: Full Scrape (All Data)

```bash
# Edit config.json, set "max_pages": 200000
python scraper_bulk.py
```

### Example 4: View Scraped Data

```bash
python view_data.py
```

---

## ğŸ¯ Scraping Workflow

```
1. Load Website â†’ 2. Click Search â†’ 3. Extract Page Data
                                              â†“
        6. Save Final File â† 5. Next Page â† 4. Save Checkpoint
                                 â†“
                            (Repeat 3-5)
```

---

## âš ï¸ Important Notes

### For Long-Running Scrapes

1. **Disable Sleep Mode**: Prevent computer from sleeping
   ```bash
   powercfg /change standby-timeout-ac 0
   ```

2. **Stable Internet**: Ensure reliable connection

3. **Monitor Progress**: Check terminal output regularly

4. **Keep Checkpoint Files**: They're your backup if something goes wrong

### Stopping Safely

Press `Ctrl + C` to stop. The scraper will:
- Save all collected data
- Create an "interrupted" file
- Close browser cleanly

---

## ğŸ” Data Viewing

View your scraped data quickly:

```bash
python view_data.py
```

Output:
```
Reading: ccr_final_20251004_144851.xlsx

Total rows: 44
Total columns: 9

Columns:
  1. Ø±Ù‚Ù… Ø§Ù„ØªØ³Ø¬ÙŠÙ„
  2. Ø§Ù„Ù…Ø­Ø§ÙØ¸Ø©
  3. ØªØ§Ø±ÙŠØ® Ø§Ù„ØªØ³Ø¬ÙŠÙ„
  ...
```

---

## ğŸ› Troubleshooting

### Issue: Browser closes unexpectedly
**Solution**: Increase `page_load_delay` in config.json to 3-4 seconds

### Issue: "Stale element" errors
**Solution**: Normal behavior, scraper handles this automatically

### Issue: Slow scraping speed
**Solution**: 
- Close unnecessary applications
- Check internet connection
- Reduce `page_load_delay` to 1 (if connection is stable)

### Issue: No data extracted
**Solution**: Website might be down or blocking. Wait and retry.

---

## ğŸ“Š Sample Output

| Ø±Ù‚Ù… Ø§Ù„ØªØ³Ø¬ÙŠÙ„ | Ø§Ù„Ù…Ø­Ø§ÙØ¸Ø© | ØªØ§Ø±ÙŠØ® Ø§Ù„ØªØ³Ø¬ÙŠÙ„ | Ù…Ø§Ù„Ùƒ Ø§Ù„Ù…Ø¤Ø³Ø³Ø© | Ø±Ø£Ø³ Ø§Ù„Ù…Ø§Ù„ Ø§Ù„Ø­Ø§Ù„ÙŠ | Ø§Ù„Ø­Ø§Ù„Ø© |
|------------|---------|--------------|-------------|----------------|--------|
| 100160 | Ø¹Ù…Ø§Ù† | 09/06/1997 | Ù…Ø­Ù…ÙˆØ¯ Ø¹Ø¨Ø¯Ø§Ù„ÙØªØ§Ø­ | 500 | Ù…ÙØ³ÙˆØ®Ù‡ |
| 100506 | Ø¹Ù…Ø§Ù† | 02/07/1997 | Ù…Ø­Ù…ÙˆØ¯ Ø¹Ø¨Ø¯Ø§Ù„ÙØªØ§Ø­ | 500 | Ù‚Ø§Ø¦Ù…Ù‡ |
| 399576 | Ø¹Ù…Ø§Ù† | 30/10/2017 | Ù…Ø±Ø§Ø¯ ØºØ§Ù„Ø¨ | 1,000 | Ù‚Ø§Ø¦Ù…Ø© / Ù…Ø­Ø¬ÙˆØ²Ø© |

---

## ğŸ” Legal & Ethical Use

- This scraper is for **legitimate data collection** purposes only
- Respect the website's terms of service
- Avoid excessive request rates that could impact server performance
- Use collected data responsibly and in compliance with local regulations

---

## ğŸ¤ Contributing

Improvements and suggestions are welcome! To contribute:

1. Fork the repository
2. Create a feature branch
3. Make your changes
4. Submit a pull request

---

## ğŸ“ Technical Details

### Technologies Used

- **Selenium**: Web automation and browser control
- **Pandas**: Data manipulation and Excel export
- **OpenPyXL**: Excel file handling
- **Chrome WebDriver**: Browser automation driver

### Browser Requirements

- Google Chrome (latest version recommended)
- ChromeDriver (automatically managed by webdriver-manager)

---

## ğŸ“ Support

For issues or questions:
1. Check `START_HERE.md` for detailed guide
2. Review troubleshooting section above
3. Check terminal output for error messages

---

## ğŸ“… Version History

### Version 1.0 (October 2025)
- Initial release
- Bulk scraping with checkpoint system
- Arabic text support
- Configurable via JSON
- Progress tracking
- Multiple export formats

---

## âš¡ Performance Tips

1. **Internet Speed**: Faster connection = faster scraping
2. **Page Delay**: Reduce to 1-2 seconds if connection is stable
3. **Checkpoint Interval**: Adjust in `scraper_bulk.py` (line 27)
4. **Headless Mode**: Uncomment in code for slightly better performance

---

## ğŸ“¦ Dependencies

```
selenium>=4.15.0
pandas>=2.0.0
openpyxl>=3.1.0
webdriver-manager>=4.0.0
```

See `requirements.txt` for complete list.

---

## ğŸ“ Learning Resources

- [Selenium Documentation](https://selenium-python.readthedocs.io/)
- [Pandas Documentation](https://pandas.pydata.org/docs/)
- [Web Scraping Best Practices](https://www.scrapehero.com/web-scraping-best-practices/)

---

## âœ… Tested On

- Windows 10/11
- Python 3.8, 3.9, 3.10, 3.11, 3.13
- Chrome 120+

---

## ğŸ“„ License

This project is provided as-is for data collection purposes. Please ensure compliance with local laws and website terms of service when using this tool.

---

**Built with â¤ï¸ for efficient data collection**

**Last Updated**: October 5, 2025
